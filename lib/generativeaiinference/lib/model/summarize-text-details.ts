/**
 * Generative AI Service Inference API
 * OCI Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases for text generation, summarization, and text embeddings. 

Use the Generative AI service inference API to access your custom model endpoints, or to try the out-of-the-box models to [generate text](#/en/generative-ai-inference/latest/GenerateTextResult/GenerateText), [summarize](#/en/generative-ai-inference/latest/SummarizeTextResult/SummarizeText), and [create text embeddings](#/en/generative-ai-inference/latest/EmbedTextResult/EmbedText).

To use a Generative AI custom model for inference, you must first create an endpoint for that model. Use the [Generative AI service management API](/#/en/generative-ai/latest/) to [create a custom model](#/en/generative-ai/latest/Model/) by fine-tuning an out-of-the-box model, or a previous version of a custom model, using your own data. Fine-tune the custom model on a  [fine-tuning dedicated AI cluster](#/en/generative-ai/latest/DedicatedAiCluster/). Then, create a [hosting dedicated AI cluster](#/en/generative-ai/latest/DedicatedAiCluster/) with an [endpoint](#/en/generative-ai/latest/Endpoint/) to host your custom model. For resource management in the Generative AI service, use the [Generative AI service management API](/#/en/generative-ai/latest/).

To learn more about the service, see the [Generative AI documentation](/iaas/Content/generative-ai/home.htm).

 * OpenAPI spec version: 20231130
 * 
 *
 * NOTE: This class is auto generated by OracleSDKGenerator.
 * Do not edit the class manually.
 *
 * Copyright (c) 2020, 2024, Oracle and/or its affiliates.  All rights reserved.
 * This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.
 */

import * as model from "../model";
import common = require("oci-common");

/**
 * Details for the request to summarize text.
 */
export interface SummarizeTextDetails {
  /**
   * The input string to be summarized.
   */
  "input": string;
  "servingMode": model.DedicatedServingMode | model.OnDemandServingMode;
  /**
   * The OCID of compartment that the user is authorized to use to call into the Generative AI service.
   */
  "compartmentId": string;
  /**
   * Whether or not to include the original inputs in the response.
   */
  "isEcho"?: boolean;
  /**
    * A number that sets the randomness of the generated output. Lower temperatures mean less random generations. 
* <p>
Use lower numbers for tasks with a correct answer such as question answering or summarizing. High temperatures can generate hallucinations or factually incorrect information. Start with temperatures lower than 1.0, and increase the temperature for more creative outputs, as you regenerate the prompts to refine the outputs.
*  Note: Numbers greater than Number.MAX_SAFE_INTEGER will result in rounding issues.
    */
  "temperature"?: number;
  /**
   * A free-form instruction for modifying how the summaries get generated. Should complete the sentence \"Generate a summary _\". For example, \"focusing on the next steps\" or \"written by Yoda\".
   */
  "additionalCommand"?: string;
  /**
   * Indicates the approximate length of the summary. If \"AUTO\" is selected, the best option will be picked based on the input text.
   */
  "length"?: SummarizeTextDetails.Length;
  /**
   * Indicates the style in which the summary will be delivered - in a free form paragraph or in bullet points. If \"AUTO\" is selected, the best option will be picked based on the input text.
   */
  "format"?: SummarizeTextDetails.Format;
  /**
   * Controls how close to the original text the summary is. High extractiveness summaries will lean towards reusing sentences verbatim, while low extractiveness summaries will tend to paraphrase more.
   */
  "extractiveness"?: SummarizeTextDetails.Extractiveness;
}

export namespace SummarizeTextDetails {
  export enum Length {
    Short = "SHORT",
    Medium = "MEDIUM",
    Long = "LONG",
    Auto = "AUTO"
  }

  export enum Format {
    Paragraph = "PARAGRAPH",
    Bullets = "BULLETS",
    Auto = "AUTO"
  }

  export enum Extractiveness {
    Low = "LOW",
    Medium = "MEDIUM",
    High = "HIGH",
    Auto = "AUTO"
  }

  export function getJsonObj(obj: SummarizeTextDetails): object {
    const jsonObj = {
      ...obj,
      ...{
        "servingMode": obj.servingMode ? model.ServingMode.getJsonObj(obj.servingMode) : undefined
      }
    };

    return jsonObj;
  }
  export function getDeserializedJsonObj(obj: SummarizeTextDetails): object {
    const jsonObj = {
      ...obj,
      ...{
        "servingMode": obj.servingMode
          ? model.ServingMode.getDeserializedJsonObj(obj.servingMode)
          : undefined
      }
    };

    return jsonObj;
  }
}
