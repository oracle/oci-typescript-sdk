/**
 * Generative AI Service Inference API
 * OCI Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases for text generation, summarization, and text embeddings. 

Use the Generative AI service inference API to access your custom model endpoints, or to try the out-of-the-box models to {@link #eNGenerative-ai-inferenceLatestChatResultChat(ENGenerative-ai-inferenceLatestChatResultChatRequest) eNGenerative-ai-inferenceLatestChatResultChat}, {@link #eNGenerative-ai-inferenceLatestGenerateTextResultGenerateText(ENGenerative-ai-inferenceLatestGenerateTextResultGenerateTextRequest) eNGenerative-ai-inferenceLatestGenerateTextResultGenerateText}, {@link #eNGenerative-ai-inferenceLatestSummarizeTextResultSummarizeText(ENGenerative-ai-inferenceLatestSummarizeTextResultSummarizeTextRequest) eNGenerative-ai-inferenceLatestSummarizeTextResultSummarizeText}, and {@link #eNGenerative-ai-inferenceLatestEmbedTextResultEmbedText(ENGenerative-ai-inferenceLatestEmbedTextResultEmbedTextRequest) eNGenerative-ai-inferenceLatestEmbedTextResultEmbedText}.

To use a Generative AI custom model for inference, you must first create an endpoint for that model. Use the {@link #eNGenerative-aiLatest(ENGenerative-aiLatestRequest) eNGenerative-aiLatest} to {@link #eNGenerative-aiLatestModel(ENGenerative-aiLatestModelRequest) eNGenerative-aiLatestModel} by fine-tuning an out-of-the-box model, or a previous version of a custom model, using your own data. Fine-tune the custom model on a {@link #eNGenerative-aiLatestDedicatedAiCluster(ENGenerative-aiLatestDedicatedAiClusterRequest) eNGenerative-aiLatestDedicatedAiCluster}. Then, create a {@link #eNGenerative-aiLatestDedicatedAiCluster(ENGenerative-aiLatestDedicatedAiClusterRequest) eNGenerative-aiLatestDedicatedAiCluster} with an {@link Endpoint} to host your custom model. For resource management in the Generative AI service, use the {@link #eNGenerative-aiLatest(ENGenerative-aiLatestRequest) eNGenerative-aiLatest}.

To learn more about the service, see the [Generative AI documentation](https://docs.oracle.com/iaas/Content/generative-ai/home.htm).

 * OpenAPI spec version: 20231130
 * 
 *
 * NOTE: This class is auto generated by OracleSDKGenerator.
 * Do not edit the class manually.
 *
 * Copyright (c) 2020, 2026, Oracle and/or its affiliates.  All rights reserved.
 * This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.
 */

import * as model from "../model";
import common = require("oci-common");

/**
 * The response to the chat conversation.
 */
export interface CohereChatResponseV2 extends model.BaseChatResponse {
  /**
   * Unique identifier for the generated reply
   */
  "id": string;
  "message": model.CohereAssistantMessageV2;
  /**
   * Why the generation stopped.
   */
  "finishReason": CohereChatResponseV2.FinishReason;
  /**
   * The log probabilities of the generated tokens.
   */
  "logProbabilities"?: Array<model.LogProbability>;
  /**
   * If there is an error during the streaming scenario, then the {@code errorMessage} parameter contains details for the error.
   */
  "errorMessage"?: string;
  "usage"?: model.Usage;

  "apiFormat": string;
}

export namespace CohereChatResponseV2 {
  export enum FinishReason {
    Complete = "COMPLETE",
    StopSequence = "STOP_SEQUENCE",
    MaxTokens = "MAX_TOKENS",
    ToolCall = "TOOL_CALL",
    Error = "ERROR",
    /**
     * This value is used if a service returns a value for this enum that is not recognized by this
     * version of the SDK.
     */
    UnknownValue = "UNKNOWN_VALUE"
  }

  export function getJsonObj(obj: CohereChatResponseV2, isParentJsonObj?: boolean): object {
    const jsonObj = {
      ...(isParentJsonObj ? obj : (model.BaseChatResponse.getJsonObj(obj) as CohereChatResponseV2)),
      ...{
        "message": obj.message ? model.CohereAssistantMessageV2.getJsonObj(obj.message) : undefined,

        "logProbabilities": obj.logProbabilities
          ? obj.logProbabilities.map(item => {
              return model.LogProbability.getJsonObj(item);
            })
          : undefined,

        "usage": obj.usage ? model.Usage.getJsonObj(obj.usage) : undefined
      }
    };

    return jsonObj;
  }
  export const apiFormat = "COHEREV2";
  export function getDeserializedJsonObj(
    obj: CohereChatResponseV2,
    isParentJsonObj?: boolean
  ): object {
    const jsonObj = {
      ...(isParentJsonObj
        ? obj
        : (model.BaseChatResponse.getDeserializedJsonObj(obj) as CohereChatResponseV2)),
      ...{
        "message": obj.message
          ? model.CohereAssistantMessageV2.getDeserializedJsonObj(obj.message)
          : undefined,

        "logProbabilities": obj.logProbabilities
          ? obj.logProbabilities.map(item => {
              return model.LogProbability.getDeserializedJsonObj(item);
            })
          : undefined,

        "usage": obj.usage ? model.Usage.getDeserializedJsonObj(obj.usage) : undefined
      }
    };

    return jsonObj;
  }
}
